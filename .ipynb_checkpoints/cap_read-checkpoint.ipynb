{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating motives for land Use change in Tennessee\n",
    "## Massih Forootan\n",
    "\n",
    "### Overview\n",
    "\n",
    "* Farmlands area has decreased during the 2002 â€“ 2012 according to USDA Census.\n",
    "* Meanwhile, the land used for residential/commercial is growing.\n",
    "* The pattern of the change is not the same among different counties.\n",
    "\n",
    "### Question:\n",
    "##### What factor(s) are contributing to the change in farm land use?\n",
    " \n",
    "### Hypotheses: \n",
    "\n",
    "The metrics that impact the change in farmlands are believed to be or related to the following features, and will therefore be put on test:\n",
    "\n",
    "1. Farms that make **less dollar amounts** due to mismanagement and/or marketing bottlenecks will gradually receive more preference for changing land use.\n",
    "2. Farms that are **smaller in area** (eg after being split among next generation) will become less profitable, encouraging the land owner to consider changing land use\n",
    "\n",
    "   In both cases, the __market value__ for residential/commercial properties in the same area, hand in hand with __big industries__ can act as an incentive or a barrier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow:\n",
    "\n",
    "## 1. Read the data:\n",
    "\n",
    "Some data was collected from PDF files available online, using **[Tabula Software](https://tabula.technology/)**.\n",
    "\n",
    "### 1.1. Data for agricultural land use and product values:\n",
    "Collected from USDA Cencus for Agriculture [2012](https://www.agcensus.usda.gov/Publications/2012/Full_Report/Volume_1,_Chapter_2_US_State_Level/) and [2007](https://www.agcensus.usda.gov/Publications/2007/Full_Report/Volume_1,_Chapter_2_US_State_Level/)\n",
    "\n",
    "### 1.2. Data for urbanization and population metrics\n",
    "Collected from [Tennessee Census for Urbanization and Housing](https://www.census.gov/prod/cen2010/cph-2-44.pdf)\n",
    "\n",
    "\n",
    "### 1.3. Data for home sales and prices\n",
    "Collected from [Tennessee Home Development Agency](https://thda.org/research-planning/home-sales-price-by-county)\n",
    "\n",
    "### 1.4. Data for location of big industries headquartered in Tennessee\n",
    "Collected from [Ranker](https://www.ranker.com/list/companies-headquartered-in-tennessee/the-working-man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes latitude and longitude from two points and gives distance in miles\n",
    "# 0.0175 is pi/180 (1 degree in radian)\n",
    "# 3959 is earth radius in miles\n",
    "\n",
    "def euclid(y1, x1, y2, x2):\n",
    "    deltax = (x1 - x2) * 0.0175 * np.cos(y1 * 0.0175) * 3959\n",
    "    deltay = (y1 - y2) * 0.0175 * 3959\n",
    "    dist= np.sqrt(deltax**2 + deltay**2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes a string and process it as follow:\n",
    "# 1. Takes out all the non-alphanumeric characters\n",
    "# 2. Converts the spaces -of any length- into underscores\n",
    "# 3. Lowercase alphabets\n",
    "# Note: NANs converted into zero-length string to avoid error\n",
    "\n",
    "def unitext(text_in):\n",
    "    if pd.isnull(text_in) == False:\n",
    "        text_in == text_in\n",
    "    else:\n",
    "        text_in = ' '\n",
    "    \n",
    "    text_01 = re.sub('[^a-zA-Z0-9\\s]','',text_in) #Takes out all the non-alphanumeric characters\n",
    "    text_02 = re.sub('\\s+','_',text_01) # Converts the spaces -of any length- into underscores\n",
    "    text_03 = str.lower(text_02)\n",
    "    text_04 = re.sub('\\_$','',text_03) # Trims ending space characters\n",
    "    text_05 = re.sub('^\\_','',text_04) # Trims begining space characters\n",
    "    return text_05    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Reading raw spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CSVs\n",
    "tab04_df = pd.read_csv('data/table_4.csv', encoding = \"utf-8\", names=list('abcdefghijklmnopqrst'))\n",
    "tab05_df = pd.read_csv('data/table_5.csv')\n",
    "tab12_df = pd.read_csv('data/tabula-12.csv', header=None)\n",
    "tab13_df = pd.read_csv('data/tabula-13.csv', header=None)\n",
    "tab071_df = pd.read_csv('data/table_71.csv')\n",
    "tab072_df = pd.read_csv('data/table_72.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XLSX\n",
    "zip_us_df = pd.read_excel('data/zip_code_database.xlsx')\n",
    "hq_df = pd.read_excel('data/hqs.xlsx', header=None)\n",
    "sold_df = pd.read_excel('data/18.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1. Population and Households in counties\n",
    "tab04_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove blank rows\n",
    "\n",
    "#Replace NaNs with 0s\n",
    "tab04_df.fillna(0, inplace=True)\n",
    "\n",
    "# open a list for blank rows\n",
    "blank_rows=[]\n",
    "\n",
    "# this loop makes a list of blank rows\n",
    "for i in np.arange(tab04_df.shape[0]):\n",
    "    if tab04_df.iloc[i,0] == 0:\n",
    "        blank_rows.append(i)\n",
    "    else:\n",
    "        tab04_df.iloc[i,0] = unitext(str(tab04_df.iloc[i,0]))\n",
    "    \n",
    "# delete blank rows\n",
    "tab04_df.drop(blank_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataframe on original PDF page break\n",
    "tab04_df_1 = tab04_df[77:]\n",
    "tab04_df_2 = tab04_df[:77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "\n",
    "new_names=[]\n",
    "for i in np.arange(tab04_df_1.shape[1]):\n",
    "    if i < 8:\n",
    "        snippet = 'Population_'\n",
    "    else:\n",
    "        snippet = 'Housing_'\n",
    "    \n",
    "    new_names.append(snippet+str(tab04_df_1.iloc[1,i])) \n",
    "\n",
    "tab04_df_1.columns=new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting extra columns\n",
    "bad_cols=[]\n",
    "for i in np.arange(tab04_df_1.shape[1]):\n",
    "    if '_0' in tab04_df_1.columns.values[i]:\n",
    "        bad_cols.append(i)\n",
    "\n",
    "tab04_df_1b = tab04_df_1.drop(tab04_df_1.columns[bad_cols], axis=1)\n",
    "tab04_df_1b.columns.values[0] = 'County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Deleting extra rows\n",
    "bad_rows=[]\n",
    "for i in np.arange(tab04_df_1b.shape[0]):\n",
    "    if '_county' in tab04_df_1b.iloc[i,0]:\n",
    "        continue\n",
    "    else:\n",
    "        bad_rows.append(i)\n",
    "tab04_df_1b.drop(tab04_df_1b.index[bad_rows], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tab04_df_1b['County'] = tab04_df_1b['County'].str.replace('_county','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "\n",
    "new_names=[]\n",
    "for i in np.arange(tab04_df_2.shape[1]):\n",
    "    if i < 7:\n",
    "        snippet = 'Population_'\n",
    "    else:\n",
    "        snippet = 'Housing_'\n",
    "    \n",
    "    new_names.append(snippet+str(tab04_df_2.iloc[1,i])) \n",
    "\n",
    "tab04_df_2.columns=new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting extra columns\n",
    "bad_cols=[]\n",
    "for i in np.arange(tab04_df_2.shape[1]):\n",
    "    if '_0' in tab04_df_2.columns.values[i]:\n",
    "        bad_cols.append(i)\n",
    "\n",
    "tab04_df_2b = tab04_df_2.drop(tab04_df_2.columns[bad_cols], axis=1)\n",
    "tab04_df_2b.columns.values[0] = 'County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Deleting extra rows\n",
    "bad_rows=[]\n",
    "for i in np.arange(tab04_df_2b.shape[0]):\n",
    "    if '_county' in tab04_df_2b.iloc[i,0]:\n",
    "        continue\n",
    "    else:\n",
    "        bad_rows.append(i)\n",
    "tab04_df_2b.drop(tab04_df_2b.index[bad_rows], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tab04_df_2b['County'] = tab04_df_2b['County'].str.replace('_county','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop_hous_df = pd.concat([tab04_df_2b,tab04_df_1b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(pop_hous_df.shape[0]):\n",
    "    for j in np.arange(1, pop_hous_df.shape[1]):\n",
    "        pop_hous_df.iloc[i,j] = pop_hous_df.iloc[i,j].replace(',','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(1, pop_hous_df.shape[1]):\n",
    "    temp_col = list(pop_hous_df[pop_hous_df.columns[i]])\n",
    "    pop_hous_df[pop_hous_df.columns[i]] = pd.to_numeric(temp_col, errors='coerece')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dataframe is saved to a CSV file\n",
    "pop_hous_df.to_csv('data/pop_hous_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2. change in population density\n",
    "\n",
    "tab05_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tab05_df = tab05_df.drop(['Unnamed: 1', 'Housing'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tab05_df.columns = ['County', 'Land/sq.mi', 'Popul_Dens', 'Hous_Dens', 'Pop%00-10', 'Pop%90-00', 'Hous%80-90', 'Hous%00-10', 'Hous%90-00', 'Hous%80-90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tab05_df.drop([0,1,71,72], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tab05_df.drop([0,1,71,72], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(clean_tab05_df.shape[0]):\n",
    "    clean_tab05_df.iloc[i,0] = unitext(clean_tab05_df.iloc[i,0])\n",
    "    clean_tab05_df.iloc[i,0] = clean_tab05_df.iloc[i,0].replace('_county','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tab05_df = clean_tab05_df.replace('â€“','0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(clean_tab05_df.shape[0]):\n",
    "    for j in np.arange(1, clean_tab05_df.shape[1]):\n",
    "        clean_tab05_df.iloc[i,j] = clean_tab05_df.iloc[i,j].replace('â€“','-')\n",
    "        clean_tab05_df.iloc[i,j] = clean_tab05_df.iloc[i,j].replace(' ','')\n",
    "        clean_tab05_df.iloc[i,j] = clean_tab05_df.iloc[i,j].replace(',','')\n",
    "        clean_tab05_df.iloc[i,j] = float(clean_tab05_df.iloc[i,j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tab05_df.replace('â€“','-', inplace=True)\n",
    "clean_tab05_df.replace(' ','', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dataframe is saved to a CSV file\n",
    "# clean_tab05_df.to_csv('data/density_pct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.7.3.1. % of rural population (split on page break)\n",
    "\n",
    "tab071_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing extra columns\n",
    "clean_tab071_df = tab071_df.drop(['Unnamed: 1', 'Unnamed: 2', 'Urban', 'Unnamed: 4',\n",
    "       'Unnamed: 5', 'Rur ercenal', 'Unnamed: 7', 'Unnamed: 8'], axis = 1)\n",
    "\n",
    "# Removing extra rows\n",
    "clean_tab071_df.drop([0,1,2,3], axis = 0, inplace=True)\n",
    "\n",
    "# Setting header\n",
    "clean_tab071_df.columns = ['County', '% of Rural Population']\n",
    "\n",
    "# Cleaning the data\n",
    "\n",
    "# Remove junk characters, word 'county' and convert string to numbers\n",
    "\n",
    "for i in np.arange(clean_tab071_df.shape[0]):\n",
    "    clean_tab071_df.iloc[i,0] = unitext(clean_tab071_df.iloc[i,0]) \n",
    "    clean_tab071_df.iloc[i,0] = clean_tab071_df.iloc[i,0].replace('_county','')\n",
    "    clean_tab071_df.iloc[i,1] = float(clean_tab071_df.iloc[i,1].replace(' ',''))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.3.2. % of rural population (split on page break) \n",
    "\n",
    "tab072_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same procedure as tab071_df above\n",
    "\n",
    "clean_tab072_df = tab072_df.drop(['Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5',\n",
    "       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8'], axis = 1)\n",
    "\n",
    "clean_tab072_df.drop([0,1], axis = 0, inplace=True)\n",
    "\n",
    "clean_tab072_df.columns = ['County', '% of Rural Population']\n",
    "\n",
    "for i in np.arange(clean_tab072_df.shape[0]):\n",
    "    clean_tab072_df.iloc[i,0] = unitext(clean_tab072_df.iloc[i,0]) \n",
    "    clean_tab072_df.iloc[i,0] = clean_tab072_df.iloc[i,0].replace('_county','')\n",
    "    clean_tab072_df.iloc[i,1] = float(clean_tab072_df.iloc[i,1].replace(' ','.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pct_rural = pd.concat([clean_tab071_df, clean_tab072_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dataframe is saved to a CSV file\n",
    "# pct_rural.to_csv('data/pct_rural_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.4. Some complementary information on ZIP codes and geographical coordinations\n",
    "\n",
    "zip_tn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtering TN zipcodes\n",
    "zip_tn_df = zip_us_df[zip_us_df['state'] == 'TN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Removing extra columns\n",
    "zip_tn_df.drop(['type', 'decommissioned', 'state', 'timezone', 'area_codes','world_region', 'country', 'irs_estimated_population_2014'], axis= 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Aviod using 'zip' since it's a python reserved word\n",
    "zip_tn_df.rename(columns={\"zip\": \"zipcode\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acceptable_cities      672\n",
       "unacceptable_cities    617\n",
       "county                   1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is a bad data in county column\n",
    "zip_tn_df.isnull().sum()[zip_tn_df.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3660: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "# Marking the NaN county\n",
    "zip_tn_df['county'].fillna('?', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# A search revealed the missing county is Obion\n",
    "zip_tn_df.loc[16779,'county'] = 'obion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Standardizing city and county names\n",
    "for i in np.arange(zip_tn_df.shape[0]):\n",
    "    zip_tn_df.iloc[i,1] = unitext(zip_tn_df.iloc[i,1])\n",
    "    zip_tn_df.iloc[i,4] = unitext(zip_tn_df.iloc[i,4])\n",
    "    zip_tn_df.iloc[i,4] = zip_tn_df.iloc[i,4].replace('_county','')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dataframe is saved to a CSV file\n",
    "# zip_tn_df.to_csv('data/zip_tn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.5. Industrial headquarters in Tennessee\n",
    "\n",
    "hq_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the headers\n",
    "hq_df['lat'] = ''\n",
    "hq_df['long'] = ''\n",
    "hq_df.columns = ['idx', 'Company', 'City', 'Size', 'Lat', 'Long']\n",
    "hq_df.drop(['idx'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some rows lack data. Removed\n",
    "bad_rows = list(hq_df[hq_df['City'].isnull()].index)\n",
    "\n",
    "hq_df.drop(bad_rows, axis=0, inplace=True)\n",
    "\n",
    "for i in np.arange(hq_df.shape[0]):\n",
    "    hq_df.iloc[i,1] = unitext(hq_df.iloc[i,1])\n",
    "\n",
    "hq_coor = pd.merge(hq_df, zip_tn_df, how='left', left_on='City', right_on='primary_city')\n",
    "\n",
    "hq_coor.drop_duplicates(subset='Company', keep='first', inplace=True)\n",
    "\n",
    "hq_coor.drop(['City', 'Size', 'Lat', 'Long', 'zipcode', 'primary_city','acceptable_cities', 'unacceptable_cities', 'county'], axis=1, inplace=True)\n",
    "\n",
    "bad_rows = list(hq_coor[hq_coor['latitude'].isnull() | hq_coor['longitude'].isnull()].index)\n",
    "hq_coor.drop(bad_rows, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "county_coor = zip_tn_df.drop_duplicates(subset='county', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "county_coor.drop(['zipcode', 'primary_city', 'acceptable_cities', 'unacceptable_cities'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "county_coor.sort_values(by='county', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "county_coor['Avg_Dist'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(county_coor.shape[0]):\n",
    "    dist_sum = 0\n",
    "    for j in np.arange(hq_coor.shape[0]):\n",
    "        dist_sum = dist_sum + euclid(county_coor.iloc[i,1], county_coor.iloc[i,2], hq_coor.iloc[i,1], hq_coor.iloc[i,2])\n",
    "    county_coor.iloc[i,3] = dist_sum\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "county_coor.to_csv('data/indus_dist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.6. Number of houses and median price over years\n",
    "\n",
    "sold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "\n",
    "new_names=[]\n",
    "for i in np.arange(sold_df.shape[1]):\n",
    "    if i < 12:\n",
    "        snippet = 'Houses_sold_'\n",
    "    else:\n",
    "        snippet = 'Median_Price_'\n",
    "    \n",
    "    new_names.append(snippet+str(sold_df.iloc[0,i])) \n",
    "\n",
    "\n",
    "sold_df.columns=new_names\n",
    "\n",
    "# removing extra rows\n",
    "sold_df.drop([0,1,2,3,99,100,101],axis=0, inplace=True)\n",
    "\n",
    "# More cleaning and renaming on columns\n",
    "sold_df.drop(['Houses_sold_nan'], axis=1, inplace=True)\n",
    "\n",
    "sold_df.rename(columns={'Houses_sold_ ': 'County'}, inplace=True)\n",
    "\n",
    "for i in np.arange(sold_df.shape[0]):\n",
    "    sold_df.iloc[i,0] = unitext(sold_df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dataframe is saved to a CSV file\n",
    "# sold_df.to_csv('data/home_sold.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.7.1. Area and number of lands subdivided into price and area within counties (2007 & 2012)\n",
    "\n",
    "tab12_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing rows that only contain NaN\n",
    "nan_rows=[]\n",
    "for i in np.arange(tab12_df.shape[0]):\n",
    "    check_sum = tab12_df.iloc[i].isnull().sum()\n",
    "    if check_sum == 14: # All NaN\n",
    "        nan_rows.append(i)\n",
    "tab12_df.drop(nan_rows,axis=0, inplace=True)        \n",
    "\n",
    "tab12_df.reset_index(inplace=True)\n",
    "tab12_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Standardize row headers\n",
    "for i in np.arange(tab12_df.shape[0]):\n",
    "    tab12_df.iloc[i,0] = unitext(tab12_df.iloc[i,0])\n",
    "    \n",
    "\n",
    "# Looking for the word 'item' - top left corner for transpose\n",
    "\n",
    "corners = []\n",
    "start_over = []\n",
    "\n",
    "for i in np.arange(tab12_df.shape[0]):\n",
    "    if unitext(tab12_df.iloc[i,0]) == 'item':\n",
    "        corners.append(i)\n",
    "        if unitext(tab12_df.iloc[i,1]) == 'tennessee': # Start_Over reveals that there are two tables in one dataframes\n",
    "            start_over.append(i)\n",
    "            \n",
    "\n",
    "corners.append(tab12_df.shape[0])\n",
    "\n",
    "# Start_Over reveals that there are two tables in one dataframes\n",
    "# These two tables have to be handled separately\n",
    "\n",
    "corners1=corners[:corners.index(start_over[1])+1]\n",
    "corners2=corners[corners.index(start_over[1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# First table is blank1_df\n",
    "\n",
    "blank1_df = pd.DataFrame()\n",
    "\n",
    "for i in np.arange(len(corners1)-2):\n",
    "    temp = tab12_df[corners1[i] : corners1[i+1]]\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.drop(['index'], axis=1, inplace=True)\n",
    "    temp_tran = temp.transpose()\n",
    "    blank1_df = pd.concat([blank1_df,temp_tran])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Second table is blank2_df\n",
    "\n",
    "blank2_df = pd.DataFrame()\n",
    "\n",
    "for i in np.arange(len(corners2)-2):\n",
    "    temp = tab12_df[corners2[i] : corners2[i + 1]]\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.drop(['index'], axis=1, inplace=True)\n",
    "    temp_tran = temp.transpose()\n",
    "    blank2_df = pd.concat([blank2_df,temp_tran])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BLANK1_DF \n",
    "\n",
    "# Renaming the columns\n",
    "blank1_df.columns = blank1_df.iloc[0]\n",
    "\n",
    "blank1_df.reset_index(inplace=True)\n",
    "\n",
    "blank1_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Removing rows that only contain NaN\n",
    "nan_rows=[]\n",
    "for i in np.arange(blank1_df.shape[0]):\n",
    "    if unitext(str(blank1_df.iloc[i,0])) == 'nan' or unitext(str(blank1_df.iloc[i,0])) == 'item':\n",
    "        nan_rows.append(i)\n",
    "blank1_df.drop(nan_rows,axis=0, inplace=True)        \n",
    "\n",
    "\n",
    "blank1_df.drop([1], axis=0, inplace=True)\n",
    "\n",
    "# Standardize row headers\n",
    "for i in np.arange(blank1_df.shape[0]):\n",
    "    blank1_df.iloc[i,0] = unitext(blank1_df.iloc[i,0])\n",
    "\n",
    "\n",
    "bad_cols = blank1_df.isnull().sum()[blank1_df.isnull().sum() != 0]\n",
    "\n",
    "old_names = list(blank1_df.columns)\n",
    "\n",
    "head_cols = list(bad_cols.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for i in np.arange(17,26):\n",
    "    ren_col = str(blank1_df.columns[16] + \"_\" + blank1_df.columns[i])\n",
    "    new_names.append(ren_col)\n",
    "\n",
    "old_names[17:26] = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head07 = []\n",
    "headac = []\n",
    "for i in np.arange(len(old_names)):\n",
    "    if old_names[i] == '2007':\n",
    "        head07.append(i)\n",
    "    elif old_names[i] == 'acres':\n",
    "        headac.append(i)\n",
    "    else:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in head07:\n",
    "    leftapp = old_names[i-1]\n",
    "    leftapp = leftapp[:-4]\n",
    "    old_names[i] = leftapp + old_names[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in headac:\n",
    "    leftapp = old_names[i-1]\n",
    "    leftapp = leftapp[:-5]\n",
    "    old_names[i] = leftapp + old_names[i]   \n",
    "\n",
    "blank1_df.columns = old_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for i in np.arange(29,53,2):\n",
    "    ren_col = str(blank1_df.columns[28] + \"_\" + blank1_df.columns[i])\n",
    "    new_names.append(ren_col)\n",
    "    ren_col2 = str(blank1_df.columns[28] + \"_\" + blank1_df.columns[i+1])\n",
    "    new_names.append(ren_col2)\n",
    "\n",
    "old_names[29:53] = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for i in np.arange(54,78,2):\n",
    "    ren_col = str(blank1_df.columns[53] + \"_\" + blank1_df.columns[i])\n",
    "    new_names.append(ren_col)\n",
    "    ren_col2 = str(blank1_df.columns[53] + \"_\" + blank1_df.columns[i+1])\n",
    "    new_names.append(ren_col2)\n",
    "\n",
    "old_names[54:78] = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [79,83,89]:\n",
    "    old_names[i+2] = old_names[i][:-10] + old_names[i+2]\n",
    "    old_names[i+3] = old_names[i+1][:-10] + old_names[i+3]\n",
    "    \n",
    "\n",
    "blank1_df.columns = old_names\n",
    "\n",
    "blank1_df.drop(list(bad_cols.index), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(blank1_df.shape[0]):\n",
    "    for j in np.arange(1, blank1_df.shape[1]):\n",
    "        blank1_df.iloc[i,j] = blank1_df.iloc[i,j].replace(',','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(1, blank1_df.shape[1]):\n",
    "    temp_col = list(blank1_df[blank1_df.columns[i]])\n",
    "    blank1_df[blank1_df.columns[i]] = pd.to_numeric(temp_col, errors='coerece')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank1_df.to_csv('data/farm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BLANK2_DF \n",
    "\n",
    "# Renaming the columns\n",
    "blank2_df.columns = blank2_df.iloc[0]\n",
    "\n",
    "blank2_df.reset_index(inplace=True)\n",
    "\n",
    "blank2_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Removing rows that only contain NaN\n",
    "nan_rows=[]\n",
    "for i in np.arange(blank2_df.shape[0]):\n",
    "    if unitext(str(blank2_df.iloc[i,0])) == 'nan' or unitext(str(blank2_df.iloc[i,0])) == 'item':\n",
    "        nan_rows.append(i)\n",
    "blank2_df.drop(nan_rows,axis=0, inplace=True)        \n",
    "\n",
    "\n",
    "blank2_df.drop([1], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize row headers\n",
    "for i in np.arange(blank2_df.shape[0]):\n",
    "    blank2_df.iloc[i,0] = unitext(blank2_df.iloc[i,0])\n",
    "\n",
    "\n",
    "bad_cols = blank2_df.isnull().sum()[blank2_df.isnull().sum() != 0]\n",
    "\n",
    "old_names = list(blank2_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head07 = []\n",
    "headac = []\n",
    "for i in np.arange(len(old_names)):\n",
    "    if old_names[i] == '2007':\n",
    "        head07.append(i)\n",
    "    elif old_names[i] == 'acres_2012':\n",
    "        headac.append(i)\n",
    "    else:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in headac:\n",
    "    leftapp = old_names[i-2]\n",
    "    leftapp = leftapp[:-10]\n",
    "    old_names[i] = leftapp + old_names[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in head07:\n",
    "    leftapp = old_names[i-1]\n",
    "    leftapp = leftapp[:-4]\n",
    "    old_names[i] = leftapp + old_names[i]   \n",
    "\n",
    "blank2_df.columns = old_names\n",
    "\n",
    "blank2_df.drop(list(bad_cols.index), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(blank2_df.shape[0]):\n",
    "    for j in np.arange(1, blank2_df.shape[1]):\n",
    "        blank2_df.iloc[i,j] = blank2_df.iloc[i,j].replace(',','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(1, blank2_df.shape[1]):\n",
    "    temp_col = list(blank2_df[blank2_df.columns[i]])\n",
    "    blank2_df[blank2_df.columns[i]] = pd.to_numeric(temp_col, errors='coerece')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank2_df.to_csv('data/pasture.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.7.2. Area and number of lands subdivided into price and area within counties (2002) \n",
    "\n",
    "tab13_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing rows that only contain NaN\n",
    "nan_rows=[]\n",
    "for i in np.arange(tab13_df.shape[0]):\n",
    "    check_sum = tab13_df.iloc[i].isnull().sum()\n",
    "    if check_sum == 14: # All NaN\n",
    "        nan_rows.append(i)\n",
    "tab13_df.drop(nan_rows,axis=0, inplace=True) \n",
    "\n",
    "tab13_df.reset_index(inplace=True)\n",
    "tab13_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Standardize row headers\n",
    "for i in np.arange(tab13_df.shape[0]):\n",
    "    tab13_df.iloc[i,0] = unitext(tab13_df.iloc[i,0])\n",
    "\n",
    "# Looking for the word 'item' - top left corner for transpose\n",
    "\n",
    "corners = []\n",
    "start_over = []\n",
    "\n",
    "for i in np.arange(tab13_df.shape[0]):\n",
    "    if unitext(tab13_df.iloc[i,0]) == 'item':\n",
    "        corners.append(i)\n",
    "        if unitext(tab13_df.iloc[i,1]) == 'tennessee': # Start_Over reveals that there are two tables in one dataframes\n",
    "            start_over.append(i)\n",
    "            \n",
    "\n",
    "corners.append(tab13_df.shape[0])\n",
    "\n",
    "# Start_Over reveals that there are two tables in one dataframes\n",
    "# These two tables have to be handled separately\n",
    "\n",
    "corners1=corners[:corners.index(start_over[1])+1]\n",
    "corners2=corners[corners.index(start_over[1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# First table is blank1_df\n",
    "\n",
    "blank1_df = pd.DataFrame()\n",
    "\n",
    "for i in np.arange(len(corners1)-2):\n",
    "    temp = tab13_df[corners1[i] : corners1[i+1]]\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.drop(['index'], axis=1, inplace=True)\n",
    "    temp_tran = temp.transpose()\n",
    "    blank1_df = pd.concat([blank1_df,temp_tran])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghfmhf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Second table is blank2_df\n",
    "\n",
    "blank2_df = pd.DataFrame()\n",
    "\n",
    "for i in np.arange(len(corners2)-2):\n",
    "    temp = tab13_df[corners2[i] : corners2[i + 1]]\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.drop(['index'], axis=1, inplace=True)\n",
    "    temp_tran = temp.transpose()\n",
    "    blank2_df = pd.concat([blank2_df,temp_tran])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BLANK1_DF \n",
    "\n",
    "# Renaming the columns\n",
    "blank1_df.columns = blank1_df.iloc[0]\n",
    "\n",
    "blank1_df.reset_index(inplace=True)\n",
    "\n",
    "blank1_df.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing rows that only contain NaN\n",
    "nan_rows=[]\n",
    "for i in np.arange(blank1_df.shape[0]):\n",
    "    if unitext(str(blank1_df.iloc[i,0])) == 'nan' or unitext(str(blank1_df.iloc[i,0])) == 'item':\n",
    "        nan_rows.append(i)\n",
    "blank1_df.drop(nan_rows,axis=0, inplace=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank1_df.drop([1], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize row headers\n",
    "for i in np.arange(blank1_df.shape[0]):\n",
    "    blank1_df.iloc[i,0] = unitext(blank1_df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_cols = blank1_df.isnull().sum()[blank1_df.isnull().sum() != 0]\n",
    "\n",
    "old_names = list(blank1_df.columns)\n",
    "\n",
    "head_cols = list(bad_cols.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for i in np.arange(17,26):\n",
    "    ren_col = str(blank1_df.columns[16] + \"_\" + blank1_df.columns[i])\n",
    "    new_names.append(ren_col)\n",
    "\n",
    "old_names[17:26] = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head02 = []\n",
    "headac = []\n",
    "for i in np.arange(len(old_names)):\n",
    "    if old_names[i] == '2002':\n",
    "        head02.append(i)\n",
    "    elif old_names[i] == 'acres':\n",
    "        headac.append(i)\n",
    "    else:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in head02:\n",
    "    leftapp = old_names[i-1]\n",
    "    leftapp = leftapp[:-4]\n",
    "    old_names[i] = leftapp + old_names[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in headac:\n",
    "    leftapp = old_names[i-1]\n",
    "    leftapp = leftapp[:-5]\n",
    "    old_names[i] = leftapp + old_names[i]   \n",
    "\n",
    "blank1_df.columns = old_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for i in np.arange(29,53,2):\n",
    "    ren_col = str(blank1_df.columns[28] + \"_\" + blank1_df.columns[i])\n",
    "    new_names.append(ren_col)\n",
    "    ren_col2 = str(blank1_df.columns[28] + \"_\" + blank1_df.columns[i+1])\n",
    "    new_names.append(ren_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_names[29:53] = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for i in np.arange(54,78,2):\n",
    "    ren_col = str(blank1_df.columns[53] + \"_\" + blank1_df.columns[i])\n",
    "    new_names.append(ren_col)\n",
    "    ren_col2 = str(blank1_df.columns[53] + \"_\" + blank1_df.columns[i+1])\n",
    "    new_names.append(ren_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_names[54:78] = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [79,83,87]:\n",
    "    old_names[i+2] = old_names[i][:-10] + old_names[i+2]\n",
    "    old_names[i+3] = old_names[i+1][:-10] + old_names[i+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "blank1_df.columns = old_names\n",
    "\n",
    "blank1_df.drop(list(bad_cols.index), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(blank1_df.shape[0]):\n",
    "    for j in np.arange(1, blank1_df.shape[1]):\n",
    "        blank1_df.iloc[i,j] = blank1_df.iloc[i,j].replace(',','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(1, blank1_df.shape[1]):\n",
    "    temp_col = list(blank1_df[blank1_df.columns[i]])\n",
    "    blank1_df[blank1_df.columns[i]] = pd.to_numeric(temp_col, errors='coerece')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BLANK2_DF \n",
    "\n",
    "# Renaming the columns\n",
    "blank2_df.columns = blank2_df.iloc[0]\n",
    "\n",
    "blank2_df.reset_index(inplace=True)\n",
    "\n",
    "blank2_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Removing rows that only contain NaN\n",
    "nan_rows=[]\n",
    "for i in np.arange(blank2_df.shape[0]):\n",
    "    if unitext(str(blank2_df.iloc[i,0])) == 'nan' or unitext(str(blank2_df.iloc[i,0])) == 'item':\n",
    "        nan_rows.append(i)\n",
    "blank2_df.drop(nan_rows,axis=0, inplace=True)        \n",
    "\n",
    "\n",
    "blank2_df.drop([1], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize row headers\n",
    "for i in np.arange(blank2_df.shape[0]):\n",
    "    blank2_df.iloc[i,0] = unitext(blank2_df.iloc[i,0])\n",
    "\n",
    "\n",
    "bad_cols = blank2_df.isnull().sum()[blank2_df.isnull().sum() != 0]\n",
    "\n",
    "old_names = list(blank2_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head02 = []\n",
    "headac = []\n",
    "for i in np.arange(len(old_names)):\n",
    "    if old_names[i] == '2002':\n",
    "        head02.append(i)\n",
    "    elif old_names[i] == 'acres_2007':\n",
    "        headac.append(i)\n",
    "    else:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in headac:\n",
    "    leftapp = old_names[i-2]\n",
    "    leftapp = leftapp[:-10]\n",
    "    old_names[i] = leftapp + old_names[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in head02:\n",
    "    leftapp = old_names[i-1]\n",
    "    leftapp = leftapp[:-4]\n",
    "    old_names[i] = leftapp + old_names[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank2_df.columns = old_names\n",
    "\n",
    "blank2_df.drop(list(bad_cols.index), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(blank2_df.shape[0]):\n",
    "    for j in np.arange(1, blank2_df.shape[1]):\n",
    "        blank2_df.iloc[i,j] = blank2_df.iloc[i,j].replace(',','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(1, blank2_df.shape[1]):\n",
    "    temp_col = list(blank2_df[blank2_df.columns[i]])\n",
    "    blank2_df[blank2_df.columns[i]] = pd.to_numeric(temp_col, errors='coerece')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the 2007 related columns that are in common between 2002-7 and 2007-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verify1_df = pd.read_csv('data/farm.csv', header=0)\n",
    "verify2_df = pd.read_csv('data/pasture.csv', header=0)\n",
    "verify1_df.drop(['Unnamed: 0'],axis=1, inplace=True)\n",
    "verify2_df.drop(['Unnamed: 0'],axis=1, inplace=True)\n",
    "\n",
    "ver2 = list(verify2_df.columns)\n",
    "ver1 = list(verify1_df.columns)\n",
    "\n",
    "var1 = pd.Series(blank1_df.columns)\n",
    "var2 = pd.Series(blank2_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_drop=[]\n",
    "for i in np.arange(1,blank1_df.shape[1]):\n",
    "    if list(var1.isin(ver1))[i] == True:\n",
    "        col_drop.append(blank1_df.columns[i])\n",
    "blank1_df.drop(col_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_drop=[]\n",
    "for i in np.arange(1,blank2_df.shape[1]):\n",
    "    if list(var1.isin(ver1))[i] == True:\n",
    "        col_drop.append(blank2_df.columns[i])\n",
    "blank2_df.drop(col_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank1_df.to_csv('data/farm2.csv')\n",
    "blank2_df.to_csv('data/pasture2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
