{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import*\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool, FactorRange\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.models.tickers import FixedTicker\n",
    "from bokeh.models import Arrow, OpenHead, NormalHead, VeeHead\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA as sklearnPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For labeling purpose. Create a list of PCx or Principal Component x\n",
    "def pc(num, lng):\n",
    "    if lng == 0:\n",
    "        prefix = 'PC'\n",
    "    else:\n",
    "        prefix = 'Principal Component '\n",
    "    pc_list = []\n",
    "    for i in np.arange(1, num+1):\n",
    "        pric = prefix+str(i)\n",
    "        pc_list.append(pric)\n",
    "    return pc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expected Proportion of the kth Largest Piece\n",
    "# Adobted from https://blogs.sas.com/content/iml/2017/08/02/retain-principal-components.html\n",
    "\n",
    "def exprop(n):\n",
    "    ex_list = []\n",
    "    for i in np.arange(1,n+1):\n",
    "        tempsum = 0\n",
    "        for j in np.arange(i, n+1):\n",
    "            tempsum = tempsum + (1/j)\n",
    "        cor = 100*tempsum/n\n",
    "        ex_list.append(cor)\n",
    "    return ex_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Read data from spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_input = pd.read_csv('data/cleaned.csv', index_col='County')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = list(raw_input.columns)\n",
    "counties = list(raw_input.index)\n",
    "# print(len(metrics), len(counties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Step for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "scaler_df = pd.DataFrame(scaler.fit_transform(raw_input))\n",
    "\n",
    "# Feature and Counties\n",
    "scaler_df.columns = metrics\n",
    "scaler_df.index = counties\n",
    "\n",
    "# Covariance matrix\n",
    "cov_df = pd.DataFrame(np.cov(scaler_df.T))\n",
    "\n",
    "cov_df.index = metrics\n",
    "cov_df.columns = metrics\n",
    "\n",
    "# Perform eigendecomposition on covariance matrix\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_df)\n",
    "\n",
    "# Eigenvalues in dataframe\n",
    "eigvals_df=pd.DataFrame(eig_vals)\n",
    "eigvals_df.columns = ['Eigenvalue']\n",
    "eigvals_df.index = pc(eigvals_df.shape[0], 1)\n",
    "\n",
    "\n",
    "# Eigenvectors in dataframe\n",
    "eigvecs_df=pd.DataFrame(eig_vecs)\n",
    "eigvecs_df.columns=pc(eig_vecs.shape[1],0)\n",
    "eigvecs_df.index = metrics\n",
    "\n",
    "\n",
    "# Create a PCA model with 30 components: the count of useful PCs will be selected later\n",
    "pca = sklearnPCA(n_components=30)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaler_df)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaler_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corrmat = scaler_df.corr()\n",
    "sns.clustermap(corrmat, vmax=.8, square=True, cmap=\"RdBu_r\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p-values for correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rows, cols = scaler_df.shape[0], scaler_df.shape[1]\n",
    "rmat = np.ones(shape=(cols, cols))\n",
    "pmat = np.ones(shape=(cols, cols))\n",
    "for i in range(cols):\n",
    "    for j in range(i+1, cols):\n",
    "        r, p = pearsonr(scaler_df.iloc[:,i], scaler_df.iloc[:,j])\n",
    "        rmat[i, j] = rmat[j, i] = r\n",
    "        pmat[i, j] = pmat[j, i] = p\n",
    "\n",
    "rmat_df = pd.DataFrame(rmat)\n",
    "pmat_df = pd.DataFrame(pmat)\n",
    "\n",
    "pmat_df.index = metrics\n",
    "pmat_df.columns = metrics\n",
    "pmat_df\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(pmat_df, vmax=.8, square=True, cmap=\"Blues\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scree plot values: determine how many components sufficiently explain variations.\n",
    "# Expected Proportion of the kth Largest Piece\n",
    "# Adobted from https://blogs.sas.com/content/iml/2017/08/02/retain-principal-components.html\n",
    "\n",
    "\n",
    "vari= eigvals_df=pd.DataFrame(100* pca.explained_variance_ratio_)\n",
    "vari['1'] = ''\n",
    "vari['2'] = ''\n",
    "vari.iloc[0,1] = vari.iloc[0,0]\n",
    "vari.iloc[0,2] = eig_vals[0].real\n",
    "for i in np.arange(1, vari.shape[0]):\n",
    "    vari.iloc[i,1] = vari.iloc[i,0] + vari.iloc[i-1,1]\n",
    "    vari.iloc[i,2] = eig_vals[i].real\n",
    "    \n",
    "vari.columns = ['Exp. Variance (%)', 'Cumulative (%)', 'Eigenvalue']\n",
    "vari.index = pc(vari.shape[0],0)\n",
    "vari['Broken Stick Threshold'] = exprop(vari.shape[0])\n",
    "#vari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "\n",
    "# drawing charts and labels\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20,6]\n",
    "\n",
    "exvar = plt.plot(np.arange(1, vari.shape[0]+1),vari['Exp. Variance (%)'], color = 'green', linewidth = 1, marker ='o')\n",
    "acvar = plt.plot(np.arange(1, vari.shape[0]+1),vari['Cumulative (%)'], color='red', linewidth = 1, marker ='o' )\n",
    "br_st = plt.plot(np.arange(1, vari.shape[0]+1),vari['Broken Stick Threshold'], color = 'violet', linewidth = 2, linestyle=\"--\")\n",
    "eival = plt.bar(np.arange(1, vari.shape[0]+1),vari['Eigenvalue'], color = 'lightblue')\n",
    "\n",
    "\n",
    "plt.xlabel('Principal Components')\n",
    "plt.xticks(np.arange(1, vari.shape[0]+1))\n",
    "plt.yticks(np.arange(10, 110, 10))\n",
    "\n",
    "# first_legend = plt.legend(handles=[exvar,first_legend],loc=1)\n",
    "# ax = plt.gca().add_artist(first_legend)\n",
    "plt.legend()\n",
    "# plt.legend(handles=[eival,first_legend])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This line filters significant according to broken stick method. (PCs 28-30 are probably neglible)\n",
    "\n",
    "vari[vari['Exp. Variance (%)'] > vari['Broken Stick Threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on scree plot result, the dataframe contains 3 PCs for the counties\n",
    "\n",
    "pcaft_df = pd.DataFrame(pca_features)\n",
    "pcaft_df.columns = pc(pcaft_df.shape[1],0)\n",
    "pcaft_df.index = counties\n",
    "pcaft_df.drop(list(pcaft_df.columns[np.arange(3,pcaft_df.shape[1])]), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Share of features in PCs\n",
    "\n",
    "pcacom_df = pd.DataFrame(pca.components_.T)\n",
    "pcacom_df.columns = pc(pcacom_df.shape[1],0)\n",
    "pcacom_df.index = metrics\n",
    "pcacom_df.drop(list(pcacom_df.columns[np.arange(3,pcacom_df.shape[1])]), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pcaft_df.to_csv('data/plots.csv') \n",
    "# pcacom_df.to_csv('data/vectors.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which feature have higher contribution to selected PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_dict = pd.read_csv('data/Col_Dict.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacom1 = pd.DataFrame(pcacom_df['PC1'])\n",
    "pcacom2 = pd.DataFrame(pcacom_df['PC2'])\n",
    "pcacom3 = pd.DataFrame(pcacom_df['PC3'])\n",
    "\n",
    "pcacom1['PC1_abs']=np.absolute(pcacom_df['PC1'])\n",
    "pcacom2['PC2_abs']=np.absolute(pcacom_df['PC2'])\n",
    "pcacom3['PC3_abs']=np.absolute(pcacom_df['PC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacom1 = pcacom1.merge(feat_dict, left_index=True, right_on='brief_header', how='left')\n",
    "pcacom1.drop(['PC1','brief_header','Ref'],axis=1, inplace=True)\n",
    "\n",
    "pcacom1.sort_values(by='PC1_abs', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacom2 = pcacom2.merge(feat_dict, left_index=True, right_on='brief_header', how='left')\n",
    "pcacom2.drop(['PC2','brief_header','Ref'],axis=1, inplace=True)\n",
    "\n",
    "pcacom2.sort_values(by='PC2_abs', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacom3 = pcacom3.merge(feat_dict, left_index=True, right_on='brief_header', how='left')\n",
    "pcacom3.drop(['PC3','brief_header','Ref'],axis=1, inplace=True)\n",
    "\n",
    "pcacom3.sort_values(by='PC3_abs', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step: clustering the features and counties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
